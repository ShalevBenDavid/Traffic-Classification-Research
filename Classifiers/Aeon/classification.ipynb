{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Time Series Classification with aeon\n",
    "\n",
    "Time Series Classification (TSC) involves training a model from a collection\n",
    " of time series (real valued, ordered, data) in order to predict a target variable.\n",
    " For example, we might want to build a model that can predict whether a patient\n",
    " is sick based on their ECG reading, or a persons type of movement based on the trace\n",
    "  of the position of their hand. This notebook gives a quick guide to TSC to get you\n",
    "  started using aeon time series classifiers. If you can use scikit-learn, it should\n",
    "   be easy, because the basic usage is identical.\n",
    "\n",
    "<img src=\"img/tsc.png\" width=\"600\" alt=\"time series classification\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Classification Notebooks\n",
    "\n",
    "This note book gives an overview of TSC. More specific notebooks on TSC are base on\n",
    "the type of representation or transformation they use:\n",
    "\n",
    "- [Convolution based](convolution_based.ipynb)\n",
    "- [Deep learning](deep_learning.ipynb)\n",
    "- [Dictionary based](dictionary_based.ipynb)\n",
    "- [Distance based](distance_based.ipynb)\n",
    "- [Feature based](feature_based.ipynb)\n",
    "- [Interval based](interval_based.ipynb)\n",
    "- [Shapelet based](shapelet_based.ipynb)\n",
    "- [Hybrid](hybrid.ipynb)\n",
    "- [Early classification](early_classification.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Data Storage and Problem Types\n",
    "\n",
    "Time series can be univariate (each observation is a single value) or multivariate\n",
    "(each observation is a vector). For example, an ECG reading from a single\n",
    "sensor is a univariate series, but a motion trace of from a smart watch would be\n",
    "multivariate, with at least three dimensions (x,y,z co-ordinates). The image above is\n",
    " a univariate problem: each series has its own label. The dimension of the time\n",
    " series instance is also often called the channel. We recommend storing time series\n",
    " in 3D numpy array of shape `(n_instances, n_channels, n_timepoints)` and\n",
    " where  possible our single problem loaders will return a\n",
    " 3D numpy. Unequal length classification problems are stored in a list of 2D numpy\n",
    " arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # Plotting and data loading imports used in this notebook\n",
    "# import warnings\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# from aeon.datasets import load_arrow_head, load_basic_motions\n",
    "\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# arrow, arrow_labels = load_arrow_head(split=\"train\")\n",
    "# motions, motions_labels = load_basic_motions(split=\"train\")\n",
    "# print(f\"ArrowHead series of type {type(arrow)} and shape {arrow.shape}\")\n",
    "# print(f\"Motions type {type(motions)} of shape {motions_labels.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "trafficArr = np.loadtxt(open(\"traffic.csv\", \"rb\"), delimiter=\",\")\n",
    "labels = np.loadtxt(open(\"labels.csv\", \"rb\"), delimiter=\",\", dtype=str)\n",
    "\n",
    "# convert labels to numpy array\n",
    "labels = np.array(labels)\n",
    "\n",
    "# train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(trafficArr, labels, test_size=0.2, random_state=42,stratify=labels)\n",
    "\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We tend to use 3D numpy even if the data is univariate, although all classifiers work\n",
    " with shape (instance, time point), currently some transformers do not work correctly\n",
    "  with 2D arrays. If your series are unequal length, have missing values or are\n",
    "  sampled at irregular time intervals, you should read the note book\n",
    "  on [data preprocessing](../utils/preprocessing.ipynb).\n",
    "\n",
    "The UCR/UEA [TSC dataset archive](https://timeseriesclassification.com/) contains a\n",
    "large number of example TSC problems that have been used thousands of times in the\n",
    "literature to assess TSC algorithms. These datasets have certain characteristics that\n",
    "influence what data structure we use to store them in memory.\n",
    "\n",
    "Most datasets in the archive contain time series all the same length. For example,\n",
    "the [ArrowHead dataset](https://timeseriesclassification.com/description.php?Dataset=ArrowHead) we have just loaded consists of outlines of the images of\n",
    "arrow heads. The classification of projectile points is an important topic in anthropology.\n",
    "\n",
    "<img src=\"../img/arrow-heads.png\" width=\"600\" alt=\"arrow heads\">\n",
    "\n",
    "The shapes of the projectile points are converted into a sequence using the\n",
    "angle-based method as described in this [blog post](https://izbicki.me/blog/converting-images-into-time-series-for-data-mining.html) about converting images into time series for data mining.\n",
    "\n",
    "<img src=\"img/from-shapes-to-time-series.png\" width=\"600\" alt=\"from shapes to time series\">\n",
    "\n",
    "Each instance consists of a single time series (i.e. the problem is univariate) of\n",
    "equal length and a class label based on shape distinctions such as the presence and\n",
    "location of a notch in the arrow. The data set consists of 210 instances, by default split into 36 train and 175 test instances.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to use a standard `sklearn` classifier for univariate, equal length\n",
    "classification problems, but it is unlikely to perform as well as bespoke time series\n",
    " classifiers, since `sklearn` classifiers ignore the sequence information in the variables.\n",
    "\n",
    "To apply `sklearn` classifiers directly, the data needs to be reshaped into a 2D\n",
    "numpy array. We also offer the ability to load univariate TSC problems directly in 2D\n",
    " arrays. Please note that currently,\n",
    " some Transformers treat a single multivariate time series in a numpy array as shape\n",
    " `(n_timepoints, n_channels)` rather than `(n_channels,n_timepoints)`\n",
    "  do not work correctly  with 2D numpy classification problems, so we recommend using\n",
    "   3D numpyof shape `(n_channels, 1, n_timepoints)` for univariate series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9557453416149069\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   Google Doc       0.99      0.96      0.97       244\n",
      " Google Drive       0.95      0.96      0.95       327\n",
      " Google Music       0.91      0.90      0.91       118\n",
      "Google Search       0.99      0.99      0.99       383\n",
      "      Youtube       0.89      0.92      0.90       216\n",
      "\n",
      "     accuracy                           0.96      1288\n",
      "    macro avg       0.95      0.94      0.95      1288\n",
      " weighted avg       0.96      0.96      0.96      1288\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "rand_forest = RandomForestClassifier(n_estimators=100)\n",
    "rand_forest.fit(X_train, y_train)\n",
    "y_pred = rand_forest.predict(X_test)\n",
    "\n",
    "print(\"Accuracy:\",accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Time Series Classifiers in aeon\n",
    "\n",
    "`aeon` contains the state of the art in time series classifiers in the package\n",
    "`classification`. These are grouped based on the data representation used to find\n",
    "discriminatory features. We provide a separate notebook for each of type:\n",
    "[convolution based](convolution_based.ipynb), [deep learning](deep_learning.ipynb), [distance based](distance_based.ipynb), [dictionary based](dictionary_based.ipynb),\n",
    "[feature_based](feature_based.ipynb), [hybrid](hybrid.ipynb), [interval based](interval_based.ipynb), and [shapelet based](shapelet_based.ipynb). We also\n",
    "provide some\n",
    "standard classifiers not available in scikit learn in the sklearn package.\n",
    "We show the simplest use cases for classifiers and demonstrate how to build bespoke\n",
    "pipelines for time series classification. An accurate and relatively\n",
    "fast classifier is the [ROCKET](https://link.springer.com/article/10.1007/s10618-020-00701-z) classifier. ROCKET is a convolution based algorithm\n",
    "described in detail in the [convolution based](convolution_based.ipynb) note book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chenhajaj/miniconda3/envs/aeon-env/lib/python3.11/site-packages/aeon/transformations/collection/tsfresh.py:13: UserWarning: No module named 'tsfresh'. 'tsfresh' is a soft dependency and not included in the base aeon installation. Please run: `pip install tsfresh` to install the tsfresh package. To install all soft dependencies, run: `pip install aeon[all_extras]`\n",
      "  _check_soft_dependencies(\"tsfresh\", severity=\"warning\")\n",
      "OMP: Info #276: omp_set_nested routine deprecated, please use omp_set_max_active_levels instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9091614906832298\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   Google Doc       0.99      0.97      0.98       244\n",
      " Google Drive       0.97      0.87      0.92       327\n",
      " Google Music       0.78      0.75      0.77       118\n",
      "Google Search       0.98      0.96      0.97       383\n",
      "      Youtube       0.72      0.89      0.80       216\n",
      "\n",
      "     accuracy                           0.91      1288\n",
      "    macro avg       0.89      0.89      0.89      1288\n",
      " weighted avg       0.92      0.91      0.91      1288\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from aeon.classification.convolution_based import RocketClassifier\n",
    "\n",
    "rocket = RocketClassifier(num_kernels=2000)\n",
    "rocket.fit(X_train, y_train)\n",
    "y_pred = rocket.predict(X_test)\n",
    "\n",
    "print(\"Accuracy:\",accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Another accurate classifier for time series classification is version 2 of the\n",
    "[HIVE-COTE](https://link.springer.com/article/10.1007/s10994-021-06057-9) algorithm.\n",
    "(HC2) is described in the [hybrid notebook](hybrid.ipynb) notebook. HC2 is relatively\n",
    "slow\n",
    "on small problems like these examples. However, it can be\n",
    "configured with an approximate maximum run time as follows (it may take a bit longer\n",
    "than 12 seconds to run this cell, very short times are approximate since there is a\n",
    "minimum amount of work the classifier needs to do):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9363354037267081\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   Google Doc       0.99      0.97      0.98       244\n",
      " Google Drive       0.95      0.94      0.95       327\n",
      " Google Music       0.82      0.81      0.82       118\n",
      "Google Search       0.99      0.97      0.98       383\n",
      "      Youtube       0.84      0.89      0.86       216\n",
      "\n",
      "     accuracy                           0.94      1288\n",
      "    macro avg       0.92      0.92      0.92      1288\n",
      " weighted avg       0.94      0.94      0.94      1288\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from aeon.classification.hybrid import HIVECOTEV2\n",
    "\n",
    "hc2 = HIVECOTEV2(time_limit_in_minutes=0.2)\n",
    "hc2.fit(X_train, y_train)\n",
    "y_pred = hc2.predict(X_test)\n",
    "\n",
    "print(\"Accuracy:\",accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Multivariate Classification\n",
    "To use ``sklearn`` classifiers directly on multivariate data, one option is to flatten\n",
    "the data so that the 3D array `(n_cases, n_channels, series_length)` becomes a 2D array\n",
    "of shape `(n_cases, n_channels*series_length)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# motions_test, motions_test_labels = load_basic_motions(split=\"test\")\n",
    "# motions2d = motions.reshape(motions.shape[0], motions.shape[1] * motions.shape[2])\n",
    "# motions2d_test = motions_test.reshape(\n",
    "#     motions_test.shape[0], motions_test.shape[1] * motions_test.shape[2]\n",
    "# )\n",
    "# rand_forest.fit(motions2d, motions_labels)\n",
    "# y_pred = rand_forest.predict(motions2d_test)\n",
    "# accuracy_score(motions_test_labels, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "However, many ``aeon`` classifiers, including ROCKET and HC2, are configured to\n",
    "work with multivariate input. This works exactly like univariate classification. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# rocket.fit(motions, motions_labels)\n",
    "# y_pred = rocket.predict(motions_test)\n",
    "# accuracy_score(motions_test_labels, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "A list of classifiers capable of handling multivariate classification can be obtained\n",
    " with this code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chenhajaj/miniconda3/envs/aeon-env/lib/python3.11/site-packages/aeon/networks/cnn.py:9: UserWarning: tensorflow and tensorflow-probability are required for deep learning and probabilistic functionality in `aeon`. To install these dependencies, run: `pip install aeon[dl]`\n",
      "  _check_dl_dependencies(severity=\"warning\")\n",
      "/Users/chenhajaj/miniconda3/envs/aeon-env/lib/python3.11/site-packages/aeon/networks/encoder.py:9: UserWarning: tensorflow and tensorflow-probability are required for deep learning and probabilistic functionality in `aeon`. To install these dependencies, run: `pip install aeon[dl]`\n",
      "  _check_dl_dependencies(severity=\"warning\")\n",
      "/Users/chenhajaj/miniconda3/envs/aeon-env/lib/python3.11/site-packages/aeon/networks/fcn.py:9: UserWarning: tensorflow and tensorflow-probability are required for deep learning and probabilistic functionality in `aeon`. To install these dependencies, run: `pip install aeon[dl]`\n",
      "  _check_dl_dependencies(severity=\"warning\")\n",
      "/Users/chenhajaj/miniconda3/envs/aeon-env/lib/python3.11/site-packages/aeon/networks/inception.py:8: UserWarning: tensorflow and tensorflow-probability are required for deep learning and probabilistic functionality in `aeon`. To install these dependencies, run: `pip install aeon[dl]`\n",
      "  _check_dl_dependencies(severity=\"warning\")\n",
      "/Users/chenhajaj/miniconda3/envs/aeon-env/lib/python3.11/site-packages/aeon/networks/mlp.py:9: UserWarning: tensorflow and tensorflow-probability are required for deep learning and probabilistic functionality in `aeon`. To install these dependencies, run: `pip install aeon[dl]`\n",
      "  _check_dl_dependencies(severity=\"warning\")\n",
      "/Users/chenhajaj/miniconda3/envs/aeon-env/lib/python3.11/site-packages/aeon/networks/tapnet.py:18: UserWarning: No module named 'keras_self_attention'. 'keras-self-attention' is a soft dependency and not included in the base aeon installation. Please run: `pip install keras-self-attention` to install the keras-self-attention package. To install all soft dependencies, run: `pip install aeon[all_extras]`\n",
      "  _check_soft_dependencies(\n",
      "/Users/chenhajaj/miniconda3/envs/aeon-env/lib/python3.11/site-packages/aeon/networks/tapnet.py:23: UserWarning: tensorflow and tensorflow-probability are required for deep learning and probabilistic functionality in `aeon`. To install these dependencies, run: `pip install aeon[dl]`\n",
      "  _check_dl_dependencies(severity=\"warning\")\n",
      "/Users/chenhajaj/miniconda3/envs/aeon-env/lib/python3.11/site-packages/aeon/annotation/adapters/_pyod.py:16: UserWarning: No module named 'pyod'. 'pyod' is a soft dependency and not included in the base aeon installation. Please run: `pip install pyod` to install the pyod package. To install all soft dependencies, run: `pip install aeon[all_extras]`\n",
      "  _check_soft_dependencies(\"pyod\", severity=\"warning\")\n",
      "/Users/chenhajaj/miniconda3/envs/aeon-env/lib/python3.11/site-packages/numba/core/decorators.py:282: RuntimeWarning: nopython is set for njit and is ignored\n",
      "  warnings.warn('nopython is set for njit and is ignored', RuntimeWarning)\n",
      "/Users/chenhajaj/miniconda3/envs/aeon-env/lib/python3.11/site-packages/aeon/annotation/hmm_learn/gaussian.py:17: UserWarning: No module named 'hmmlearn'. 'hmmlearn.hmm' is a soft dependency and not included in the base aeon installation. Please run: `pip install hmmlearn.hmm` to install the hmmlearn.hmm package. To install all soft dependencies, run: `pip install aeon[all_extras]`\n",
      "  _check_soft_dependencies(\"hmmlearn.hmm\", severity=\"warning\")\n",
      "/Users/chenhajaj/miniconda3/envs/aeon-env/lib/python3.11/site-packages/aeon/annotation/hmm_learn/gmm.py:18: UserWarning: No module named 'hmmlearn'. 'hmmlearn.hmm' is a soft dependency and not included in the base aeon installation. Please run: `pip install hmmlearn.hmm` to install the hmmlearn.hmm package. To install all soft dependencies, run: `pip install aeon[all_extras]`\n",
      "  _check_soft_dependencies(\"hmmlearn.hmm\", severity=\"warning\")\n",
      "/Users/chenhajaj/miniconda3/envs/aeon-env/lib/python3.11/site-packages/aeon/annotation/hmm_learn/poisson.py:17: UserWarning: No module named 'hmmlearn'. 'hmmlearn.hmm' is a soft dependency and not included in the base aeon installation. Please run: `pip install hmmlearn.hmm` to install the hmmlearn.hmm package. To install all soft dependencies, run: `pip install aeon[all_extras]`\n",
      "  _check_soft_dependencies(\"hmmlearn.hmm\", severity=\"warning\")\n",
      "/Users/chenhajaj/miniconda3/envs/aeon-env/lib/python3.11/site-packages/aeon/clustering/k_shapes.py:11: UserWarning: No module named 'tslearn'. 'tslearn' is a soft dependency and not included in the base aeon installation. Please run: `pip install tslearn` to install the tslearn package. To install all soft dependencies, run: `pip install aeon[all_extras]`\n",
      "  _check_soft_dependencies(\"tslearn\", severity=\"warning\")\n",
      "/Users/chenhajaj/miniconda3/envs/aeon-env/lib/python3.11/site-packages/aeon/clustering/kernel_k_means.py:11: UserWarning: No module named 'tslearn'. 'tslearn' is a soft dependency and not included in the base aeon installation. Please run: `pip install tslearn` to install the tslearn package. To install all soft dependencies, run: `pip install aeon[all_extras]`\n",
      "  _check_soft_dependencies(\"tslearn\", severity=\"warning\")\n",
      "/Users/chenhajaj/miniconda3/envs/aeon-env/lib/python3.11/site-packages/aeon/forecasting/arima.py:12: UserWarning: No module named 'pmdarima'. 'pmdarima' is a soft dependency and not included in the base aeon installation. Please run: `pip install pmdarima` to install the pmdarima package. To install all soft dependencies, run: `pip install aeon[all_extras]`\n",
      "  _check_soft_dependencies(\"pmdarima\", severity=\"warning\")\n",
      "/Users/chenhajaj/miniconda3/envs/aeon-env/lib/python3.11/site-packages/aeon/forecasting/bats.py:18: UserWarning: No module named 'tbats'. 'tbats' is a soft dependency and not included in the base aeon installation. Please run: `pip install tbats` to install the tbats package. To install all soft dependencies, run: `pip install aeon[all_extras]`\n",
      "  _check_soft_dependencies(\"tbats\", severity=\"warning\")\n",
      "/Users/chenhajaj/miniconda3/envs/aeon-env/lib/python3.11/site-packages/aeon/forecasting/fbprophet.py:14: UserWarning: No module named 'prophet'. 'prophet' is a soft dependency and not included in the base aeon installation. Please run: `pip install prophet` to install the prophet package. To install all soft dependencies, run: `pip install aeon[all_extras]`\n",
      "  _check_soft_dependencies(\"prophet\", severity=\"warning\")\n",
      "/Users/chenhajaj/miniconda3/envs/aeon-env/lib/python3.11/site-packages/aeon/forecasting/statsforecast.py:14: UserWarning: No module named 'statsforecast'. 'statsforecast' is a soft dependency and not included in the base aeon installation. Please run: `pip install statsforecast` to install the statsforecast package. To install all soft dependencies, run: `pip install aeon[all_extras]`\n",
      "  _check_soft_dependencies(\"statsforecast\", severity=\"warning\")\n",
      "/Users/chenhajaj/miniconda3/envs/aeon-env/lib/python3.11/site-packages/aeon/forecasting/statsforecast.py:15: UserWarning: This functionality requires package 'pandas<2.0.0' to be present in the python environment, with version <2.0.0, but incompatible version 2.0.3 was found. \n",
      "  _check_soft_dependencies(\"pandas<2.0.0\", severity=\"warning\")\n",
      "/Users/chenhajaj/miniconda3/envs/aeon-env/lib/python3.11/site-packages/aeon/forecasting/tbats.py:18: UserWarning: No module named 'tbats'. 'tbats' is a soft dependency and not included in the base aeon installation. Please run: `pip install tbats` to install the tbats package. To install all soft dependencies, run: `pip install aeon[all_extras]`\n",
      "  _check_soft_dependencies(\"tbats\", severity=\"warning\")\n",
      "/Users/chenhajaj/miniconda3/envs/aeon-env/lib/python3.11/site-packages/aeon/forecasting/varmax.py:13: UserWarning: This functionality requires package 'pandas<2.0.0' to be present in the python environment, with version <2.0.0, but incompatible version 2.0.3 was found. \n",
      "  _check_soft_dependencies(\"pandas<2.0.0\", severity=\"warning\")\n",
      "/Users/chenhajaj/miniconda3/envs/aeon-env/lib/python3.11/site-packages/aeon/transformations/series/bkfilter.py:20: UserWarning: No module named 'statsmodels'. 'statsmodels' is a soft dependency and not included in the base aeon installation. Please run: `pip install statsmodels` to install the statsmodels package. To install all soft dependencies, run: `pip install aeon[all_extras]`\n",
      "  _check_soft_dependencies(\"statsmodels\", severity=\"warning\")\n",
      "/Users/chenhajaj/miniconda3/envs/aeon-env/lib/python3.11/site-packages/aeon/transformations/series/filter.py:12: UserWarning: No module named 'mne'. 'mne' is a soft dependency and not included in the base aeon installation. Please run: `pip install mne` to install the mne package. To install all soft dependencies, run: `pip install aeon[all_extras]`\n",
      "  _check_soft_dependencies(\"mne\", severity=\"warning\")\n",
      "/Users/chenhajaj/miniconda3/envs/aeon-env/lib/python3.11/site-packages/aeon/transformations/series/kalman_filter.py:23: UserWarning: No module named 'filterpy'. 'filterpy' is a soft dependency and not included in the base aeon installation. Please run: `pip install filterpy` to install the filterpy package. To install all soft dependencies, run: `pip install aeon[all_extras]`\n",
      "  _check_soft_dependencies(\"filterpy\", severity=\"warning\")\n",
      "/Users/chenhajaj/miniconda3/envs/aeon-env/lib/python3.11/site-packages/aeon/transformations/series/matrix_profile.py:12: UserWarning: No module named 'stumpy'. 'stumpy' is a soft dependency and not included in the base aeon installation. Please run: `pip install stumpy` to install the stumpy package. To install all soft dependencies, run: `pip install aeon[all_extras]`\n",
      "  _check_soft_dependencies(\"stumpy\", severity=\"warning\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>estimator</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [name, estimator]\n",
       "Index: []"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from aeon.registry import all_estimators\n",
    "\n",
    "all_estimators(\n",
    "    filter_tags={\"capability:multivariate\": True},\n",
    "    estimator_types=\"classifier\",\n",
    "    as_dataframe=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "An alternative for MTSC is to build a univariate classifier on each dimension, then\n",
    "ensemble. Dimension ensembling can be easily done via ``ColumnEnsembleClassifier``\n",
    "which fits classifiers independently to specified dimensions, then\n",
    "combines predictions through a voting scheme. The interface is\n",
    "similar to the ``ColumnTransformer`` from `sklearn`. The example below builds a DrCIF\n",
    "classifier on the first channel and a RocketClassifier on the fourth and fifth\n",
    "dimensions, ignoring the second, third and sixth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from aeon.classification.compose import ChannelEnsembleClassifier\n",
    "# from aeon.classification.interval_based import DrCIFClassifier\n",
    "\n",
    "# cls = ChannelEnsembleClassifier(\n",
    "#     estimators=[\n",
    "#         (\"DrCIF0\", DrCIFClassifier(n_estimators=5, n_intervals=2), [0]),\n",
    "#         (\"ROCKET3\", RocketClassifier(num_kernels=1000), [3, 4]),\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "# cls.fit(motions, motions_labels)\n",
    "# y_pred = cls.predict(motions_test)\n",
    "\n",
    "# accuracy_score(motions_test_labels, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## sklearn Compatibility\n",
    "\n",
    "`aeon` classifiers are compatible with `sklearn` model selection and\n",
    "composition tools using `aeon` data formats. For example, cross-validation can\n",
    "be performed using the `sklearn` `cross_val_score` and `KFold` functionality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.13913043, 0.3136646 , 0.87826087, 0.95152268])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "\n",
    "cross_val_score(rocket, trafficArr, y=labels, cv=KFold(n_splits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Parameter tuning can be done using `sklearn` `GridSearchCV`. For example, we can tune\n",
    " the _k_ and distance measure for a K-NN classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9433229813664596"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from aeon.classification.distance_based import KNeighborsTimeSeriesClassifier\n",
    "\n",
    "knn = KNeighborsTimeSeriesClassifier()\n",
    "param_grid = {\"n_neighbors\": [1, 5], \"distance\": [\"euclidean\", \"dtw\"]}\n",
    "parameter_tuning_method = GridSearchCV(knn, param_grid, cv=KFold(n_splits=4))\n",
    "\n",
    "parameter_tuning_method.fit(X_train, y_train)\n",
    "y_pred = parameter_tuning_method.predict(X_test)\n",
    "\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Probability calibration is possible with the `sklearn` `CalibratedClassifierCV`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9510869565217391"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "from aeon.classification.interval_based import DrCIFClassifier\n",
    "\n",
    "calibrated_drcif = CalibratedClassifierCV(\n",
    "    estimator=DrCIFClassifier(n_estimators=10, n_intervals=5), cv=4\n",
    ")\n",
    "\n",
    "calibrated_drcif.fit(X_train, y_train)\n",
    "y_pred = calibrated_drcif.predict(X_test)\n",
    "\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Background info and references for classifiers used here\n",
    "\n",
    "#### KNeighborsTimeSeriesClassifier\n",
    "\n",
    "One nearest neighbour (1-NN) classification with Dynamic Time Warping (DTW) is one of the oldest TSC approaches, and is commonly used as a performance benchmark.\n",
    "\n",
    "#### RocketClassifier\n",
    "The RocketClassifier is based on a pipeline combination of the ROCKET transformation (transformations.panel.rocket) and the sklearn RidgeClassifierCV classifier. The RocketClassifier is configurable to use variants MiniRocket and MultiRocket. ROCKET is based on generating random convolutional kernels. A large number are generated, then a linear classifier is built on the output.\n",
    "\n",
    "[1] Dempster, Angus, François Petitjean, and Geoffrey I. Webb. \"Rocket: exceptionally fast and accurate time series classification using random convolutional kernels.\" Data Mining and Knowledge Discovery (2020)\n",
    "[arXiv version](https://arxiv.org/abs/1910.13051)\n",
    "[DAMI 2020](https://link.springer.com/article/10.1007/s10618-020-00701-z)\n",
    "\n",
    "#### DrCIF\n",
    "The Diverse Representation Canonical Interval Forest Classifier (DrCIF) is an interval based classifier. The algorithm takes multiple randomised intervals from each series and extracts a range of features. These features are used to build a decision tree, which in turn are ensembled into a decision tree forest, in the style of a random forest.\n",
    "\n",
    "Original CIF classifier:\n",
    "[2] Matthew Middlehurst and James Large and Anthony Bagnall. \"The Canonical Interval Forest (CIF) Classifier for Time Series Classification.\" IEEE International Conference on Big Data (2020)\n",
    "[arXiv version](https://arxiv.org/abs/2008.09172)\n",
    "[IEEE BigData (2020)](https://ieeexplore.ieee.org/abstract/document/9378424?casa_token=8g_IG5MLJZ4AAAAA:ItxW0bY4eCRwfdV9kLvf-8a8X73UFCYUGU9D19PwrHigjivLJVchxHwkM3Btn7vvlOJ_0HiLRa3LCA)\n",
    "\n",
    "The DrCIF adjustment was proposed in [3].\n",
    "\n",
    "#### HIVE-COTE 2.0 (HC2)\n",
    "The HIerarchical VotE Collective of Transformation-based Ensembles is a meta ensemble that combines classifiers built on different representations. Version 2  combines DrCIF, TDE, an ensemble of RocketClassifiers called the Arsenal and the  ShapeletTransformClassifier. It is one of the most accurate classifiers on the UCR and UEA time series archives.\n",
    "\n",
    "[3] Middlehurst, Matthew, James Large, Michael Flynn, Jason Lines, Aaron Bostrom, and Anthony Bagnall. \"HIVE-COTE 2.0: a new meta ensemble for time series classification.\" Machine Learning (2021)\n",
    "[ML 2021](https://link.springer.com/article/10.1007/s10994-021-06057-9)\n",
    "\n",
    "#### Catch22\n",
    "\n",
    "The CAnonical Time-series CHaracteristics (Catch22) are a set of 22 informative and low redundancy features extracted from time series data. The features were filtered from 4791 features in the `hctsa` toolkit.\n",
    "\n",
    "[4] Lubba, Carl H., Sarab S. Sethi, Philip Knaute, Simon R. Schultz, Ben D. Fulcher, and Nick S. Jones. \"catch22: Canonical time-series characteristics.\" Data Mining and Knowledge Discovery (2019)\n",
    "[DAMI 2019](https://link.springer.com/article/10.1007/s10618-019-00647-x)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9d800c14abb2bd109b7479fe8830174a66f0a4a77373f77c2c7334932e1a4922"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
